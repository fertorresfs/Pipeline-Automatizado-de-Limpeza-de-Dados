{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "194EN-lR-0P821yeFy3TBj_O1GJkX0Y2m",
      "authorship_tag": "ABX9TyP95q0tS+bzRe85B6gDOvWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fertorresfs/Pipeline-Automatizado-de-Limpeza-de-Dados/blob/main/pipeline_csnu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuração, Bibliotecas e Funções de Pipeline"
      ],
      "metadata": {
        "id": "HSzSaXHKQux0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNWENN4UL5zc",
        "outputId": "06cd09e0-bde6-452c-b8a9-7b6ffad6b398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuração de logging concluída. O log será gravado em '/content/drive/MyDrive/Pipelines/CSNU/log/pipeline_ofac.log'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import re\n",
        "from io import StringIO\n",
        "import os\n",
        "\n",
        "# --- 1. Configuração de Logging ---\n",
        "LOG_FILE = '/content/drive/MyDrive/Pipelines/CSNU/log/pipeline_ofac.log'\n",
        "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
        "logging.basicConfig(filename=LOG_FILE, level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
        "print(f\"Configuração de logging concluída. O log será gravado em '{os.path.abspath(LOG_FILE)}'.\")\n",
        "\n",
        "# --- DEFINIÇÃO DO CABEÇALHO DO SDN.CSV (CRÍTICO) ---\n",
        "# Nomes das 12 colunas do SDN.CSV da OFAC, conforme a documentação\n",
        "OFAC_SDN_HEADERS = [\n",
        "    'ent_num', 'SDN_Name', 'SDN_Type', 'Program', 'Title',\n",
        "    'Call_Sign', 'Vess_type', 'Tonnage', 'GRT', 'Vess_flag',\n",
        "    'Vess_owner', 'Remarks'\n",
        "]\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# FASE E & L (Extract & Load) - CORRIGIDA\n",
        "# ======================================================================\n",
        "\n",
        "def ingestao_csv_ofac(file_name='SDN.CSV'):\n",
        "    \"\"\"\n",
        "    Extrai o arquivo CSV da lista de sanções da OFAC, ignorando linhas de metadados\n",
        "    e aplicando o cabeçalho correto.\n",
        "    \"\"\"\n",
        "    BASE_URL = \"https://sanctionslistservice.ofac.treas.gov/api/download/\"\n",
        "    FULL_URL = f\"{BASE_URL}{file_name}\"\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Iniciando requisição GET para download do arquivo: {file_name}\")\n",
        "        response = requests.get(FULL_URL, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        csv_data = StringIO(response.text)\n",
        "\n",
        "        # --- CORREÇÃO CRÍTICA: Definir header=None e skiprows=1 ---\n",
        "        # 1. header=None: Informa ao Pandas para não usar a primeira linha como cabeçalho (já que são dados)\n",
        "        # 2. names=OFAC_SDN_HEADERS: Aplica o cabeçalho definido manualmente\n",
        "        # 3. skiprows=1: Tenta pular a primeira linha (que o log mostrou ser dados) se ela for de metadados\n",
        "        df = pd.read_csv(csv_data,\n",
        "                         na_values=['-0-'],\n",
        "                         encoding='latin1',\n",
        "                         skipinitialspace=True,\n",
        "                         header=None, # Tenta ler sem cabeçalho automático\n",
        "                         names=OFAC_SDN_HEADERS, # Define o cabeçalho manualmente\n",
        "                         skiprows=1 # Pula a primeira linha, que é um registro completo (ex: 36, AEROCARIBBEAN...)\n",
        "                        )\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        # A limpeza de colunas não é mais necessária, pois usamos 'names'\n",
        "\n",
        "        logging.info(f\"Dados do arquivo {file_name} carregados com sucesso. Linhas: {len(df)}\")\n",
        "        logging.info(f\"Colunas Finais Ingeridas: {list(df.columns)}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro inesperado durante a ingestão: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Funções de Transformação (Mantidas, pois os nomes de coluna estão CORRETOS agora)\n",
        "\n",
        "def tratar_valores_ausentes(df, colunas_numericas, estrategia='remover_linhas'):\n",
        "    df_out = df.copy()\n",
        "    if estrategia == 'remover_linhas':\n",
        "        colunas_criticas = ['ent_num', 'SDN_Name', 'Program']\n",
        "        df_out.dropna(subset=[col for col in colunas_criticas if col in df_out.columns], inplace=True)\n",
        "        logging.info(f\"Linhas com nulos em colunas críticas removidas.\")\n",
        "    elif estrategia == 'preencher_media':\n",
        "        for col in colunas_numericas:\n",
        "            if col in df_out.columns and pd.api.types.is_numeric_dtype(df_out[col]):\n",
        "                media = df_out[col].mean()\n",
        "                df_out[col].fillna(media, inplace=True)\n",
        "                logging.info(f\"Valores ausentes da coluna '{col}' preenchidos com a média: {media:.2f}\")\n",
        "    return df_out\n",
        "\n",
        "def validar_tipos(df, colunas_numericas, colunas_categoricas):\n",
        "    df_out = df.copy()\n",
        "    for col in colunas_numericas:\n",
        "        if col in df_out.columns:\n",
        "            df_out[col] = pd.to_numeric(df_out[col], errors='coerce')\n",
        "            logging.info(f\"Coluna '{col}' convertida para numérica.\")\n",
        "    for col in colunas_categoricas:\n",
        "        if col in df_out.columns:\n",
        "            df_out[col] = df_out[col].astype('category')\n",
        "            logging.info(f\"Coluna '{col}' convertida para categórica.\")\n",
        "    return df_out\n",
        "\n",
        "def limpar_e_extrair_aliases(df):\n",
        "    \"\"\"Extrai os \"Weak Aliases\" (AKAs Fracos) da coluna 'Remarks'.\"\"\"\n",
        "    df_out = df.copy()\n",
        "    coluna_name = 'SDN_Name'\n",
        "    coluna_remarks = 'Remarks' # Nome de coluna correto agora\n",
        "\n",
        "    if coluna_remarks not in df_out.columns or coluna_name not in df_out.columns:\n",
        "        # Isso não deve mais acontecer\n",
        "        logging.warning(\"Falha na identificação de colunas críticas, mesmo após renomeação manual.\")\n",
        "        df_out['SDN_Name_Clean'] = None\n",
        "        df_out['Weak_Aliases_Clean'] = None\n",
        "        return df_out\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Iniciando limpeza de texto e extração de 'Weak Aliases'.\")\n",
        "\n",
        "        df_out['SDN_Name_Clean'] = df_out[coluna_name].str.lower().str.strip()\n",
        "\n",
        "        # Regex busca strings entre aspas duplas, conforme a documentação OFAC para o CSV.\n",
        "        raw_aliases = df_out[coluna_remarks].apply(\n",
        "            lambda x: re.findall(r'\"([^\"]*)\"', str(x)) if pd.notna(x) else []\n",
        "        )\n",
        "\n",
        "        df_out['Weak_Aliases_Clean'] = raw_aliases.apply(\n",
        "             lambda aliases: [str(a).lower().strip() for a in aliases]\n",
        "        )\n",
        "\n",
        "        logging.info(\"Limpeza de texto e extração de 'Weak Aliases' concluídas.\")\n",
        "        return df_out\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro crítico na limpeza de texto/extração de aliases: {e}\")\n",
        "        df_out['SDN_Name_Clean'] = None\n",
        "        df_out['Weak_Aliases_Clean'] = None\n",
        "        return df_out\n",
        "\n",
        "def detectar_outliers(df, colunas_numericas, metodo='iqr', limiar=1.5):\n",
        "  df_out = df.copy()\n",
        "  if metodo == 'iqr':\n",
        "      for col in colunas_numericas:\n",
        "        if col in df_out.columns and pd.api.types.is_numeric_dtype(df_out[col]):\n",
        "            if col in ['ent_num']: continue\n",
        "            Q1 = df_out[col].quantile(0.25)\n",
        "            Q3 = df_out[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            limite_inferior = Q1 - (limiar * IQR)\n",
        "            limite_superior = Q3 + (limiar * IQR)\n",
        "            df_out = df_out[(df_out[col] >= limite_inferior) & (df_out[col] <= limite_superior)]\n",
        "            logging.info(f\"Outliers detectados e removidos (IQR) na coluna '{col}'.\")\n",
        "  return df_out\n",
        "\n",
        "# ======================================================================\n",
        "# VARIÁVEIS DE CONFIGURAÇÃO DO PIPELINE\n",
        "# ======================================================================\n",
        "COLUNAS_NUMERICAS_SDN = ['ent_num', 'Tonnage', 'GRT']\n",
        "COLUNAS_CATEGORICAS_SDN = ['SDN_Type', 'Program', 'Vess_type', 'Vess_flag']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execução do Pipeline ELT"
      ],
      "metadata": {
        "id": "M90MtLLQQzvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXECUÇÃO DO PIPELINE ---\n",
        "NOME_ARQUIVO_ENTRADA = 'SDN.CSV'\n",
        "NOME_ARQUIVO_SAIDA = 'dados_limpos_ofac_sdn.csv'\n",
        "COLUNAS_RELATORIO = ['ent_num', 'SDN_Name_Clean', 'Program', 'Tonnage', 'Weak_Aliases_Clean']\n",
        "\n",
        "print(f\"\\n--- INICIANDO PIPELINE ELT/LIMPEZA DE DADOS: {NOME_ARQUIVO_ENTRADA} ---\")\n",
        "\n",
        "# ======================================================================\n",
        "# PASSO 1: E (Extract) & L (Load)\n",
        "# ======================================================================\n",
        "df_dados_brutos = ingestao_csv_ofac(file_name=NOME_ARQUIVO_ENTRADA)\n",
        "\n",
        "if df_dados_brutos is None:\n",
        "    print(\"\\n[ERRO FATAL] Falha na Ingestão. Consulte o log para detalhes.\")\n",
        "else:\n",
        "    print(f\"\\n[SUCESSO] Dados brutos carregados. Linhas: {len(df_dados_brutos)}\")\n",
        "\n",
        "    df_trabalho = df_dados_brutos.copy()\n",
        "\n",
        "    # ======================================================================\n",
        "    # PASSO 2: T (Transform) - Limpeza e Padronização\n",
        "    # ======================================================================\n",
        "\n",
        "    print(\"-> [2.1] VALIDANDO E CONVERTENDO TIPOS...\")\n",
        "    df_trabalho = validar_tipos(df_trabalho, COLUNAS_NUMERICAS_SDN, COLUNAS_CATEGORICAS_SDN)\n",
        "\n",
        "    print(\"-> [2.2] TRATANDO VALORES AUSENTES...\")\n",
        "    df_trabalho = tratar_valores_ausentes(df_trabalho, COLUNAS_NUMERICAS_SDN, estrategia='remover_linhas')\n",
        "    df_trabalho = tratar_valores_ausentes(df_trabalho, ['Tonnage', 'GRT'], estrategia='preencher_media')\n",
        "\n",
        "    # Esta função deve encontrar as colunas\n",
        "    print(\"-> [2.3] EXTRAINDO E PADRONIZANDO ALIASES...\")\n",
        "    df_trabalho = limpar_e_extrair_aliases(df_trabalho)\n",
        "\n",
        "    print(\"-> [2.4] DETECTANDO OUTLIERS (IQR)...\")\n",
        "    df_trabalho = detectar_outliers(df_trabalho, ['Tonnage', 'GRT'], metodo='iqr', limiar=1.5)\n",
        "\n",
        "    # ======================================================================\n",
        "    # PASSO 3: T (Transform) - Saída e Relatório\n",
        "    # ======================================================================\n",
        "\n",
        "    print(\"\\n--- RELATÓRIO DE LIMPEZA ---\")\n",
        "    print(f\"Linhas Originais: {len(df_dados_brutos)}\")\n",
        "    print(f\"Linhas Finais (após limpeza): {len(df_trabalho)}\")\n",
        "    print(f\"Linhas Descartadas: {len(df_dados_brutos) - len(df_trabalho)}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    colunas_existentes = [col for col in COLUNAS_RELATORIO if col in df_trabalho.columns]\n",
        "\n",
        "    print(\"AMOSTRA DO DATAFRAME LIMPO:\")\n",
        "    if colunas_existentes:\n",
        "        print(df_trabalho[colunas_existentes].head())\n",
        "    else:\n",
        "        # Se a amostra falhar, pelo menos mostra o cabeçalho final para novo diagnóstico\n",
        "        print(\"Amostra de colunas de relatório falhou. Cabeçalho final do DataFrame:\")\n",
        "        print(df_trabalho.head())\n",
        "\n",
        "    df_trabalho.to_csv(NOME_ARQUIVO_SAIDA, index=False)\n",
        "    logging.info(f\"Pipeline concluído. Dados limpos salvos em '{NOME_ARQUIVO_SAIDA}' (Linhas: {len(df_trabalho)}).\")\n",
        "    print(f\"\\n[SUCESSO] Dados finais salvos em '{NOME_ARQUIVO_SAIDA}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPHyZBZKM95w",
        "outputId": "f8556543-c88f-43c6-d660-cc10c1547c3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INICIANDO PIPELINE ELT/LIMPEZA DE DADOS: SDN.CSV ---\n",
            "\n",
            "[SUCESSO] Dados brutos carregados. Linhas: 18256\n",
            "-> [2.1] VALIDANDO E CONVERTENDO TIPOS...\n",
            "-> [2.2] TRATANDO VALORES AUSENTES...\n",
            "-> [2.3] EXTRAINDO E PADRONIZANDO ALIASES...\n",
            "-> [2.4] DETECTANDO OUTLIERS (IQR)...\n",
            "\n",
            "--- RELATÓRIO DE LIMPEZA ---\n",
            "Linhas Originais: 18256\n",
            "Linhas Finais (após limpeza): 18152\n",
            "Linhas Descartadas: 104\n",
            "------------------------------\n",
            "AMOSTRA DO DATAFRAME LIMPO:\n",
            "   ent_num             SDN_Name_Clean Program     Tonnage Weak_Aliases_Clean\n",
            "0    173.0  anglo-caribbean co., ltd.    CUBA  909.285714                 []\n",
            "1    306.0     banco nacional de cuba    CUBA  909.285714                 []\n",
            "2    424.0         boutique la maison    CUBA  909.285714                 []\n",
            "3    475.0               casa de cuba    CUBA  909.285714                 []\n",
            "4    480.0               cecoex, s.a.    CUBA  909.285714                 []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1529491925.py:82: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_out[col].fillna(media, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SUCESSO] Dados finais salvos em 'dados_limpos_ofac_sdn.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuração e Funções de Pipeline (Expansão para Múltiplos Arquivos)"
      ],
      "metadata": {
        "id": "FXbqecoZU3pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import re\n",
        "from io import StringIO\n",
        "import os\n",
        "\n",
        "# --- 1. Configuração de Logging ---\n",
        "LOG_FILE = '/content/drive/MyDrive/Pipelines/CSNU/log/pipeline_ofac_mult.log'\n",
        "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
        "logging.basicConfig(filename=LOG_FILE, level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
        "print(f\"Configuração de logging concluída. O log será gravado em '{os.path.abspath(LOG_FILE)}'.\")\n",
        "\n",
        "# --- DEFINIÇÃO DOS CABEÇALHOS (CRÍTICO) ---\n",
        "# SDN.CSV (12 colunas)\n",
        "OFAC_SDN_HEADERS = [\n",
        "    'ent_num', 'SDN_Name', 'SDN_Type', 'Program', 'Title',\n",
        "    'Call_Sign', 'Vess_type', 'Tonnage', 'GRT', 'Vess_flag',\n",
        "    'Vess_owner', 'Remarks'\n",
        "]\n",
        "# ADD.CSV (6 colunas)\n",
        "OFAC_ADD_HEADERS = [\n",
        "    'Ent_num', 'Add_num', 'Address', 'City_State_Province_PostalCode',\n",
        "    'Country', 'Add_remarks'\n",
        "]\n",
        "# ALT.CSV (5 colunas)\n",
        "OFAC_ALT_HEADERS = [\n",
        "    'ent_num', 'alt_num', 'alt_type', 'alt_name', 'alt_remarks'\n",
        "]\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# FASE E & L (Extract & Load)\n",
        "# ======================================================================\n",
        "\n",
        "def ingestao_csv_ofac(file_name):\n",
        "    \"\"\"\n",
        "    Extrai o arquivo CSV da OFAC e aplica o cabeçalho e as regras de leitura corretas.\n",
        "    \"\"\"\n",
        "    if file_name == 'SDN.CSV':\n",
        "        headers = OFAC_SDN_HEADERS\n",
        "    elif file_name == 'ADD.CSV':\n",
        "        headers = OFAC_ADD_HEADERS\n",
        "    elif file_name == 'ALT.CSV':\n",
        "        headers = OFAC_ALT_HEADERS\n",
        "    else:\n",
        "        logging.error(f\"Nome de arquivo desconhecido: {file_name}\")\n",
        "        return None\n",
        "\n",
        "    BASE_URL = \"https://sanctionslistservice.ofac.treas.gov/api/download/\"\n",
        "    FULL_URL = f\"{BASE_URL}{file_name}\"\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Iniciando requisição GET para download do arquivo: {file_name}\")\n",
        "        response = requests.get(FULL_URL, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        csv_data = StringIO(response.text)\n",
        "\n",
        "        # Leitura com skiprows=1 para ignorar a primeira linha (dados de exemplo/metadados)\n",
        "        df = pd.read_csv(csv_data,\n",
        "                         na_values=['-0-'],\n",
        "                         encoding='latin1',\n",
        "                         skipinitialspace=True,\n",
        "                         header=None,\n",
        "                         names=headers,\n",
        "                         skiprows=1\n",
        "                        )\n",
        "\n",
        "        # Limpa o campo de ligação do ADD.CSV\n",
        "        if file_name == 'ADD.CSV':\n",
        "             df.rename(columns={'Ent_num': 'ent_num'}, inplace=True)\n",
        "\n",
        "        logging.info(f\"Dados do arquivo {file_name} carregados com sucesso. Linhas: {len(df)}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro inesperado durante a ingestão do {file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# FASE T (Transformação/Relatório)\n",
        "# ======================================================================\n",
        "\n",
        "def gerar_relatorio_qualidade(df_final, df_sdn_orig):\n",
        "    \"\"\"\n",
        "    Gera um relatório detalhado sobre a qualidade da limpeza e a consolidação dos dados.\n",
        "\n",
        "    Args:\n",
        "        df_final (pd.DataFrame): DataFrame consolidado (SDN + ADD + ALT).\n",
        "        df_sdn_orig (pd.DataFrame): DataFrame SDN após a ingestão, antes da limpeza.\n",
        "\n",
        "    Returns:\n",
        "        str: Relatório formatado em texto.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Metadados de Linhas\n",
        "    linhas_originais = len(df_sdn_orig)\n",
        "    linhas_finais = df_final['ent_num'].nunique() # Contagem de entidades únicas no final\n",
        "    linhas_totais_mescladas = len(df_final)\n",
        "\n",
        "    # 2. Qualidade da Limpeza\n",
        "    # Nota: Assumimos que a coluna 'Tonnage' foi preenchida com a média\n",
        "    total_nulos_tonnage_preenchido = df_sdn_orig['Tonnage'].isna().sum()\n",
        "\n",
        "    # 3. Métricas de Consolidação\n",
        "    entidades_sdn = df_final['ent_num'].nunique()\n",
        "    aliases_fortes_agregados = df_final['Strong_Aliases_Clean'].apply(len).sum()\n",
        "    aliases_fracos_agregados = df_final['Weak_Aliases_Clean'].apply(len).sum()\n",
        "\n",
        "    # Contagem média de entidades x endereços (para entender a granularidade)\n",
        "    media_enderecos_por_entidade = linhas_totais_mescladas / entidades_sdn\n",
        "\n",
        "    relatorio = f\"\"\"\n",
        "======================================================================\n",
        "         RELATÓRIO DE QUALIDADE E CONSOLIDAÇÃO DO PIPELINE OFAC\n",
        "======================================================================\n",
        "1. ESTATÍSTICAS DE INGESTÃO E LINHAS\n",
        "----------------------------------------------------------------------\n",
        "Total de Entidades SDN (Antes da Limpeza): {linhas_originais}\n",
        "Total de Entidades SDN Únicas (Após a Limpeza): {linhas_finais}\n",
        "Total de Linhas no Dataset Consolidado (Incluindo Endereços): {linhas_totais_mescladas}\n",
        "Linhas Descartadas (Nulos críticos/Outliers): {linhas_originais - linhas_finais}\n",
        "\n",
        "2. QUALIDADE DE DADOS E TRANFORMACÃO\n",
        "----------------------------------------------------------------------\n",
        "Nº de Entidades com Aliases Fracos (Weak Aliases): {df_final[df_final['Weak_Aliases_Clean'].apply(len) > 0]['ent_num'].nunique()}\n",
        "Total de Aliases Fracos (Agregado): {aliases_fracos_agregados}\n",
        "Total de Aliases Fortes (Agregado - do ALT.CSV): {aliases_fortes_agregados}\n",
        "Nº de Nulos em Tonnage PREENCHIDOS com a Média: {total_nulos_tonnage_preenchido}\n",
        "Colunas Categóricas/Numéricas Validada: SDN_Type, Program, ent_num, Tonnage, GRT\n",
        "\n",
        "3. ESTATÍSTICAS DE RELACIONAMENTO (Chave: ent_num)\n",
        "----------------------------------------------------------------------\n",
        "Média de Endereços por Entidade SDN: {media_enderecos_por_entidade:.2f}\n",
        "\"\"\"\n",
        "    logging.info(\"Relatório de Qualidade de Dados Gerado.\")\n",
        "    return relatorio\n",
        "\n",
        "def validar_tipos(df, colunas_numericas, colunas_categoricas):\n",
        "    \"\"\"Garante que as colunas estejam no tipo de dado esperado.\"\"\"\n",
        "    df_out = df.copy()\n",
        "    for col in colunas_numericas:\n",
        "        if col in df_out.columns:\n",
        "            df_out[col] = pd.to_numeric(df_out[col], errors='coerce')\n",
        "            logging.info(f\"Coluna '{col}' convertida para numérica.\")\n",
        "    for col in colunas_categoricas:\n",
        "        if col in df_out.columns:\n",
        "            df_out[col] = df_out[col].astype('category')\n",
        "            logging.info(f\"Coluna '{col}' convertida para categórica.\")\n",
        "    return df_out\n",
        "\n",
        "def tratar_valores_ausentes(df, colunas_numericas, estrategia='remover_linhas'):\n",
        "    \"\"\"Trata valores ausentes.\"\"\"\n",
        "    df_out = df.copy()\n",
        "    if estrategia == 'remover_linhas':\n",
        "        # Remoção crítica em chaves de ligação e campos de identificação\n",
        "        colunas_criticas = ['ent_num', 'SDN_Name', 'Program']\n",
        "        df_out.dropna(subset=[col for col in colunas_criticas if col in df_out.columns], inplace=True)\n",
        "        logging.info(f\"Linhas com nulos em colunas críticas ({colunas_criticas}) removidas.\")\n",
        "    elif estrategia == 'preencher_media':\n",
        "        for col in colunas_numericas:\n",
        "            if col in df_out.columns and pd.api.types.is_numeric_dtype(df_out[col]):\n",
        "                media = df_out[col].mean()\n",
        "                df_out[col].fillna(media, inplace=True)\n",
        "                logging.info(f\"Valores ausentes da coluna '{col}' preenchidos com a média: {media:.2f}\")\n",
        "    return df_out\n",
        "\n",
        "def limpar_e_extrair_aliases(df_sdn):\n",
        "    \"\"\"Extrai os \"Weak Aliases\" da coluna 'Remarks' do SDN.CSV.\"\"\"\n",
        "    df_out = df_sdn.copy()\n",
        "    coluna_name = 'SDN_Name'\n",
        "    coluna_remarks = 'Remarks'\n",
        "\n",
        "    if coluna_remarks not in df_out.columns or coluna_name not in df_out.columns:\n",
        "        logging.warning(\"Colunas SDN_Name ou Remarks não encontradas para extração de aliases fracos.\")\n",
        "        return df_out\n",
        "\n",
        "    try:\n",
        "        df_out['SDN_Name_Clean'] = df_out[coluna_name].str.lower().str.strip()\n",
        "\n",
        "        # Regex busca strings entre aspas duplas na coluna Remarks\n",
        "        raw_aliases = df_out[coluna_remarks].apply(\n",
        "            lambda x: re.findall(r'\"([^\"]*)\"', str(x)) if pd.notna(x) else []\n",
        "        )\n",
        "\n",
        "        df_out['Weak_Aliases_Clean'] = raw_aliases.apply(\n",
        "             lambda aliases: [str(a).lower().strip() for a in aliases]\n",
        "        )\n",
        "        logging.info(\"Limpeza de texto e extração de 'Weak Aliases' (Fracos) concluídas.\")\n",
        "        return df_out\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro crítico na limpeza de texto/extração de aliases: {e}\")\n",
        "        return df_out\n",
        "\n",
        "def mesclar_dados(df_sdn, df_add, df_alt):\n",
        "    \"\"\"\n",
        "    Mescla (JOIN) os DataFrames SDN, Endereços e Aliases Forte.\n",
        "    \"\"\"\n",
        "    logging.info(\"Iniciando mesclagem dos DataFrames (SDN, Endereços, Aliases).\")\n",
        "\n",
        "    # 1. Mescla SDN e Endereços (ADD)\n",
        "    # Uma entidade SDN pode ter vários endereços, então mesclamos e agregamos.\n",
        "    df_merged = pd.merge(df_sdn, df_add, on='ent_num', how='left', suffixes=('_sdn', '_add'))\n",
        "\n",
        "    # 2. Mescla o resultado com Aliases (ALT)\n",
        "    # Uma entidade SDN pode ter vários aliases fortes. Agrupamos antes de mesclar.\n",
        "    df_alt_grouped = df_alt.groupby('ent_num')['alt_name'].apply(list).reset_index(name='Strong_Aliases_Clean')\n",
        "\n",
        "    df_final = pd.merge(df_merged, df_alt_grouped, on='ent_num', how='left')\n",
        "\n",
        "    # Preenche a coluna de aliases fortes com lista vazia onde não há match\n",
        "    df_final['Strong_Aliases_Clean'].fillna(pd.Series([[]] * len(df_final)), inplace=True)\n",
        "\n",
        "    logging.info(f\"Mesclagem concluída. DataFrame final tem {len(df_final)} linhas.\")\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# VARIÁVEIS DE CONFIGURAÇÃO DO PIPELINE\n",
        "# ======================================================================\n",
        "COLUNAS_NUMERICAS_SDN = ['ent_num', 'Tonnage', 'GRT']\n",
        "COLUNAS_CATEGORICAS_SDN = ['SDN_Type', 'Program', 'Vess_type', 'Vess_flag']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PydjXTlLU2Og",
        "outputId": "baa1ad46-1d47-4a13-8d8b-bb8b3c04f30b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuração de logging concluída. O log será gravado em '/content/drive/MyDrive/Pipelines/CSNU/log/pipeline_ofac_mult.log'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execução do Pipeline ELT de Mesclagem"
      ],
      "metadata": {
        "id": "IA5PalL8U_ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXECUÇÃO DO PIPELINE EXPANDIDO ---\n",
        "NOME_ARQUIVO_SAIDA = 'dados_conformidade_ofac_consolidado.csv'\n",
        "\n",
        "print(\"\\n--- INICIANDO PIPELINE DE CONFORMIDADE (SDN + ADD + ALT) ---\")\n",
        "\n",
        "# ======================================================================\n",
        "# PASSO 1: E (Extract) & L (Load) dos Três Arquivos\n",
        "# ======================================================================\n",
        "df_sdn = ingestao_csv_ofac('SDN.CSV')\n",
        "df_add = ingestao_csv_ofac('ADD.CSV')\n",
        "df_alt = ingestao_csv_ofac('ALT.CSV')\n",
        "\n",
        "if df_sdn is None or df_add is None or df_alt is None:\n",
        "    print(\"\\n[ERRO FATAL] Falha na ingestão de um ou mais arquivos. Abortando pipeline.\")\n",
        "else:\n",
        "    print(\"\\n[SUCESSO] Todos os arquivos principais carregados.\")\n",
        "\n",
        "    # ======================================================================\n",
        "    # PASSO 2: T (Transform) - Preparação e Limpeza do SDN\n",
        "    # ======================================================================\n",
        "\n",
        "    df_sdn_trabalho = df_sdn.copy()\n",
        "\n",
        "    print(\"-> [2.1] VALIDANDO E CONVERTENDO TIPOS (SDN)...\")\n",
        "    df_sdn_trabalho = validar_tipos(df_sdn_trabalho, COLUNAS_NUMERICAS_SDN, COLUNAS_CATEGORICAS_SDN)\n",
        "\n",
        "    print(\"-> [2.2] TRATANDO VALORES AUSENTES (SDN)...\")\n",
        "    df_sdn_trabalho = tratar_valores_ausentes(df_sdn_trabalho, COLUNAS_NUMERICAS_SDN, estrategia='remover_linhas')\n",
        "    df_sdn_trabalho = tratar_valores_ausentes(df_sdn_trabalho, ['Tonnage', 'GRT'], estrategia='preencher_media')\n",
        "\n",
        "    print(\"-> [2.3] EXTRAINDO E PADRONIZANDO ALIASES FRACOS (SDN)...\")\n",
        "    df_sdn_trabalho = limpar_e_extrair_aliases(df_sdn_trabalho)\n",
        "\n",
        "    print(\"-> [2.4] DETECTANDO OUTLIERS (SDN)...\")\n",
        "    # Aplica-se apenas aos dados numéricos não-ID\n",
        "    df_sdn_trabalho = detectar_outliers(df_sdn_trabalho, ['Tonnage', 'GRT'], metodo='iqr', limiar=1.5)\n",
        "\n",
        "    # ======================================================================\n",
        "    # PASSO 3: T (Transform) - Limpeza Auxiliar e Mesclagem\n",
        "    # ======================================================================\n",
        "\n",
        "    print(\"-> [3.1] LIMPEZA E PREPARAÇÃO DOS ARQUIVOS AUXILIARES (ADD/ALT)...\")\n",
        "    # Limpeza básica em ADD/ALT (garante ID numérico para join)\n",
        "    df_add['ent_num'] = pd.to_numeric(df_add['ent_num'], errors='coerce')\n",
        "    df_alt['ent_num'] = pd.to_numeric(df_alt['ent_num'], errors='coerce')\n",
        "    df_add.dropna(subset=['ent_num'], inplace=True)\n",
        "    df_alt.dropna(subset=['ent_num'], inplace=True)\n",
        "\n",
        "    print(\"-> [3.2] MESCLAGEM DOS DADOS (SDN + ADD + ALT)...\")\n",
        "    df_final = mesclar_dados(df_sdn_trabalho, df_add, df_alt)\n",
        "\n",
        "    # ======================================================================\n",
        "    # PASSO 4: SAÍDA E RELATÓRIO DE QUALIDADE DE DADOS\n",
        "    # ======================================================================\n",
        "\n",
        "    if df_sdn is not None and df_final is not None:\n",
        "      print(\"\\n--- INICIANDO GERAÇÃO DO RELATÓRIO DE QUALIDADE DE DADOS ---\")\n",
        "\n",
        "      # É necessário passar o DF SDN ANTES da limpeza de nulos críticos\n",
        "      # para obter a contagem correta de linhas originais.\n",
        "      df_sdn_bruto_para_relatorio = ingestao_csv_ofac('SDN.CSV')\n",
        "\n",
        "      relatorio = gerar_relatorio_qualidade(df_final, df_sdn_bruto_para_relatorio)\n",
        "\n",
        "      print(relatorio)\n",
        "\n",
        "      print(\"\\n--- RELATÓRIO DE CONSOLIDAÇÃO ---\")\n",
        "      print(f\"Entidades SDN Originais: {len(df_sdn)}\")\n",
        "      print(f\"Linhas Finais (Entidade x Endereço x Alias): {len(df_final)}\")\n",
        "      print(\"-\" * 30)\n",
        "\n",
        "      print(\"AMOSTRA DO DATAFRAME CONSOLIDADO (Chaves de Conformidade):\")\n",
        "      df_colunas_conformidade = ['ent_num', 'SDN_Name_Clean', 'Program', 'Tonnage', 'Country', 'Weak_Aliases_Clean', 'Strong_Aliases_Clean']\n",
        "      print(df_final[df_final.columns.intersection(df_colunas_conformidade)].head())\n",
        "\n",
        "      df_final.to_csv(NOME_ARQUIVO_SAIDA, index=False)\n",
        "      logging.info(f\"Pipeline concluído. Dados consolidados salvos em '{NOME_ARQUIVO_SAIDA}' (Linhas: {len(df_final)}).\")\n",
        "      print(f\"\\n[SUCESSO] Base de Dados de Conformidade consolidada salva em '{NOME_ARQUIVO_SAIDA}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBzXyCnlU-Ih",
        "outputId": "67f6ad69-86d6-4d12-e4c1-703a42feca45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INICIANDO PIPELINE DE CONFORMIDADE (SDN + ADD + ALT) ---\n",
            "\n",
            "[SUCESSO] Todos os arquivos principais carregados.\n",
            "-> [2.1] VALIDANDO E CONVERTENDO TIPOS (SDN)...\n",
            "-> [2.2] TRATANDO VALORES AUSENTES (SDN)...\n",
            "-> [2.3] EXTRAINDO E PADRONIZANDO ALIASES FRACOS (SDN)...\n",
            "-> [2.4] DETECTANDO OUTLIERS (SDN)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3651638275.py:114: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_out[col].fillna(media, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> [3.1] LIMPEZA E PREPARAÇÃO DOS ARQUIVOS AUXILIARES (ADD/ALT)...\n",
            "-> [3.2] MESCLAGEM DOS DADOS (SDN + ADD + ALT)...\n",
            "\n",
            "--- RELATÓRIO DE CONSOLIDAÇÃO ---\n",
            "Entidades SDN Originais: 18256\n",
            "Linhas Finais (Entidade x Endereço x Alias): 23888\n",
            "------------------------------\n",
            "AMOSTRA DO DATAFRAME CONSOLIDADO (Chaves de Conformidade):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3651638275.py:163: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_final['Strong_Aliases_Clean'].fillna(pd.Series([[]] * len(df_final)), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ent_num Program     Tonnage             SDN_Name_Clean Weak_Aliases_Clean  \\\n",
            "0    173.0    CUBA  909.285714  anglo-caribbean co., ltd.                 []   \n",
            "1    306.0    CUBA  909.285714     banco nacional de cuba                 []   \n",
            "2    306.0    CUBA  909.285714     banco nacional de cuba                 []   \n",
            "3    306.0    CUBA  909.285714     banco nacional de cuba                 []   \n",
            "4    306.0    CUBA  909.285714     banco nacional de cuba                 []   \n",
            "\n",
            "          Country     Strong_Aliases_Clean  \n",
            "0  United Kingdom            [AVIA IMPORT]  \n",
            "1     Switzerland  [NATIONAL BANK OF CUBA]  \n",
            "2           Spain  [NATIONAL BANK OF CUBA]  \n",
            "3           Japan  [NATIONAL BANK OF CUBA]  \n",
            "4          Panama  [NATIONAL BANK OF CUBA]  \n",
            "\n",
            "[SUCESSO] Base de Dados de Conformidade consolidada salva em 'dados_conformidade_ofac_consolidado.csv'.\n"
          ]
        }
      ]
    }
  ]
}